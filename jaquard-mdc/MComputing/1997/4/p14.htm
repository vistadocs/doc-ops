<html><head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<title>Windmills -- 2.7183 Apples = 3.14159 Oranges</title></head>
<body background="../../mb.gif">
<p><img src="../../mast050.jpg"></p>

<h3>Volume 5, number 4, October 1995, pages 14-15</h3>

<h2>Wind<img src="../../wmmta.gif" width="22" height="17" align=baseline>ills<br>
2.7183 Apples = 3.14159 Oranges,
or was that the other way about?</h2>

<img src="../../EdDeMoel.jpg" width=88 height=128 align=left>

<h3>by Ed J.P.M. de Moel</h3>

<p>During my first year in college, I asked my computer science
professor what are the qualities to look for in a "good" computer
program. To my surprise, his answer was that a good program:</p>

<p><ul>
<li>produces a reasonable approximation of the intended
result,</li>
<li>can be written in a reasonable amount of time and</li>
<li>uses a reasonable amount of computer resources.</li>
</ul></p>

<p>I was flabbergasted. A "reasonable approximation" instead of a
"correct answer"? Could that really be true?</p>

<h3>Approximation</h3>

<p>Since then, I have learned that some questions have a precise
answer (like "42"), and that other answers can be approximated
arbitrarily precisely, depending on the time we want to spend
finding that result. Indeed my professor was all too proud when
he could announce that he had used the university's supercomputer
to calculate "a couple of" digits of &pi;, and that he was the first
to actually implement the latest discovered optimization, and
could actually calculate a million decimals "over the
weekend".</p>

<p>Since then, I have attended many meetings of mathematicians and
physicists, where they discussed the pros and cons of mathematical
methods and algorithms, and I think that the annual meeting of the
users of supercomputers worldwide is still filled with papers
that tell us that "last year, this subroutine still took 5 hours,
but I thought a bit longer and deeper, and now it works in 20
minutes".</p>

<p>That may sound pejorative, but I don't mean it that way. What
I do mean, is that no matter how much effort went into an
optimization, there is always a way of doing it even better.</p>

<h3>Writing Time</h3>

<p>"To be written in a reasonable amount of time". Is that really
it? Don't we want to keep trying until we have the perfect
version?</p>

<p>Of course we don't. We don't want any wrong results either,
but there are many cases where we will have to live with "as
close as we can get in the time we have".</p>

<p>Many is the time I have spent a couple of hours to figure out
how to fine-tune the lay-out of a report, so that I could fit
just that much more on one page, that the total report would be
20 pages shorter, <i>and</i> I would have a better synopsis of
the information presented. For a report that has to be prepared
every month, it makes sense to put in that kind of effort. For a
quick survey that is going to be run once and once only, such
fine-tuning is generally a waste of time.</p>

<p>And... with today's tools and ODBC connectivity: should I
really still spend much time on the formatting of tables in
reports, or should I aim at producing an SQL mapping (or failing
such a tool on the system at hand, a simple tab or comma
separated list), that can be fed directly into a popular
spreadsheet program?</p>

<h3>Resources</h3>

<p>"... and uses a reasonable amount of resources..." Well, what
is reasonable?</p>

<p>Some may feel that any effort spent on calculating any more
digits of esoteric numbers like  is a waste of time to begin
with, for others finding algorithms that make it possible to fill
a multi-terabyte database with more digits and then prove that
there still is no repeating pattern is their life's
fulfillment.</p>

<p>As far as the number &pi; is concerned, most people seem to
remember the first 10 or so digits well after their college
years, and just type the first five or six digits if ever they
need the number in a practical application. Mathematical purists
often insist on writing 4 times the arctangent of 1, and I have
seen colleagues who "included" a header file at the start of
their programs that defined a constant to the level of their
favorite precision.</p>

<p>I have yet to see software that actually calculated a value
for &pi; when it was needed in a computation. Everyone seems to have
found their favorite way of obtaining this value in a manner that
imposes a minimum strain on the computer that has to do the work
eventually.</p>

<p>Now... isn't figuring out <tt>4.0*arctan(1.0)</tt> more work
than evaluating the constant <tt>3.14159265358979</tt>? Sure it
is, but software isn't just written and executed, it is also
maintained and audited and validated. If I were to do a code
review, I'd recognize the formula immediately as correct (and the
number of typos that can be made in that formula is pretty
limited), whereas seeing that the constant number doesn't contain
any typos actually takes me about 5 to 10 times as long as it
took me to type it in the first place. Somehow, the human eye
wants to be really <i>really</i> sure that something is correct
when one is reviewing code.</p>

<p>The message: well, sometimes the time that the computer needs
is not the only thing that counts. Just keep in mind:</p>

<p><center><tt>WRITE !,LINE</tt></center></p>

<p>uses a whole lot less computer resources than</p>

<p><center><tt>DO box^%window(LINE)</tt></center></p>

<p>(assuming that the latter utility pops up a window), but most
people seem to have a really strong preference for the more
expensive variant.</p>

<h3>How fast is that computer?</h3>

<p>During my college years, I got to do a lot of work on machines
that were called super-computers in their time. The one I had
access to in the early seventies was a wonder of speed that
performed a floating point division in 5 nanoseconds.</p>

<p>Today I'm working on a Pentium that dwarfs this supercomputer
of way-back-when. So... why do I keep finding myself dissatisfied
with the performance of my Pentium, while we used to share a
"lesser" machine between over a 100 people at a time, and were
raving about its "speed"?</p>

<p>The answer, of course, lies in all the other demands that we
put on the machines of today. On the machines of the '60s and
'70s, there were no windows, text was not immediately adjusted
while we typed it, if shown at all, letters were not
proportionally spaced (in fact, we only had a couple of terminals
that knew about the existence of lower case letters), and I could
go on: today, we have much more powerful machines, and we're
using all that power for... well, mostly not the activities that
are programmed inside our own software. At some PCs, my typing is
carefully overseen by a curious paperclip in the lower right
corner, at some other PCs, my typing happens simultaneously in a
formatted window, and in a window that shows the internal codes
for my text-processor (indeed, I do use both major word
processors).</p>

<p>Now, isn't that little animated paperclip an absolute waste of
CPU cycles? So far I tend to think so, but already twice over the
past month, it has been able to answer a question I had to put to
it...</p>

<p>I still hate it, but don't be surprised if I change my mind
about it soon!</p>

<p>In short, today we have considerably more computer resources
available than we had 20 years ago. Certain usage that used to be
luxury or used to be severely frowned upon has now become common
place and fashionable, and the use of computer resources for</p>

<p><ul>
<li>spell checking while you're typing</li>
<li>help resources</li>
<li>immediate formatting</li>
<li>graphics</li>
<li>fancy lay-out and illustrations</li>
<li>favoring more readable code over more efficient code</li>
<li>including redundant information in data structures so that
validation (and, if necessary, repair) becomes easier</li>
</ul></p>

<p>has become a "reasonable" use of our resources.</p>

<hr>

<p><a href="http://www.jacquardsystems.com"><img src="../../jsr2.gif"
 align=left></a><i>Ed de Moel is past chairman of the MDC
and works with Jacquard Systems Research.
His experience includes developing software for research in medicine
and physics.
Over the past ten years, Ed's has mostly focused on the production
of tools for data management and analysis, and tools for the support
of day-to-day operation of medical systems.
Ed can be
reached <a HREF="mailto:demoel@jacquardsystems.com.">by e-mail</a>.</i></p>

</body>
</html>
